---
title: "Data Preprocessing"
author: "Yifei Sun"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

\newpage
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(visdat)
library(gridExtra)
library(mvtnorm)
library(ISLR)
```

# Transforming predictors

Although not always required, transforming the variables may lead to improvement in prediction, especially for parametric models.

For example, one may consider the Box-Cox transformation, which finds an appropriate transformation from a family of power transformation that will transform the variable as close as possible to a normal distribution. One may also consider the Yeo-Johnson transformation if the variables are not strictly positive.

```{r}
gen_data <- function(N)
{
  X <- rmvnorm(N, mean = c(1,-1), 
               sigma = matrix(c(1,.5,.5,1), ncol = 2))
  X1 <- exp(X[,1])
  X2 <- X[,2]
  X3 <- rep(1, N)
  eps <- rnorm(N, sd = .5)
  Y <- log(X1) + X2 + eps
  
  data.frame(Y = Y, X1 = X1, X2 = X2, X3 = X3)
}

set.seed(2021)
trainData <- gen_data(100)
testData <- gen_data(50)

x <- trainData[, -1]
y <- trainData[, 1]
x2 <- testData[, -1]
y2 <- testData[, 1]
```

## `preProcess` in `train()`

```{r}
fit.lm <- train(x, y,
                preProcess = c("BoxCox", "zv"),
                method = "lm",
                trControl = trainControl(method = "none"))

fit.lm

pred.lm <- predict(fit.lm, newdata = x2)

fit.lm$preProcess$bc
```

## `preProcess()`

The transformation is computed using the training data. Then it is applied to both training and test data.

```{r}
pp <- preProcess(x, method = c("BoxCox", "zv"))

# transformed predictor matrix (training)
x_pp <- predict(pp, x)

head(x_pp)

# transformed predictor matrix (test)
x2_pp <- predict(pp, x2)

head(x2_pp)
```


# Missing data

There are different mechanisms for missing data: missing completely at random (MCAR), missing at random (MAR), missing not at random (MNAR). MAR means that the missingness depends
only on the observed data; MNAR means that the missingness further depends on the missing data. The missing data mechanism determines how you handle the missing data. For example, under MAR, you may consider imputation methods; under MNAR, you may consider treating missingness as an attribute.


```{r}
gen_data <- function(N)
{
  X <- rmvnorm(N, mean = c(1,-1), 
               sigma = matrix(c(1,0.5,0.5,1), ncol = 2))
  X1 <- X[,1]
  X2 <- X[,2]
  eps <- rnorm(N, sd = .5)
  Y <- X1 + X2 + eps
  
  # which X1 observations are missing
  ind_missing <- rbinom(N, size = 1, prob = exp(X2/2)/(1+exp(X2/2)))
  
  X1m <- X1
  X1m[ind_missing == 1] <- NA
  
  data.frame(Y = Y, X1m = X1m, X2 = X2, X1 = X1)
}

set.seed(2021)

dat <- gen_data(500)
dat2 <- gen_data(100)
trainData <- dat[,1:3]
testData <- dat2[,1:3]

vis_miss(trainData)
```



## `preProcess()`

```{r}
trainX <- trainData[,c(2:3)]
knnImp <- preProcess(trainX, method = "knnImpute", k = 3)
bagImp <- preProcess(trainX, method = "bagImpute")
medImp <- preProcess(trainX, method = "medianImpute")

trainX_knn <- predict(knnImp, trainX)
trainX_bag <- predict(bagImp, trainX)
trainX_med <- predict(medImp, trainX)

testData_knn <- predict(knnImp, testData)
testData_bag <- predict(bagImp, testData)
testData_med <- predict(medImp, testData)

head(trainX)

head(trainX_med)

head(trainX_knn)

head(trainX_bag)
```


```{r, echo = FALSE}
df <- data.frame(X1 = dat$X1, 
                 X1med = trainX_med$X1m,
                 X1knn = trainX_knn$X1m,
                 X1bag = trainX_bag$X1m,
                 Y = trainData$Y,
                 X2 = trainData$X2)[is.na(trainData$X1m),]

p1 <- ggplot(df, aes(x = X1, y = X2, color = Y)) + 
  geom_point(show.legend = TRUE) +
  labs(x = "X1", y = "X2",  title = "No missingness", color = "Y") + 
  scale_color_gradient(low = "green", high = "red", 
                       na.value = "blue", guide = "legend") +
  theme_minimal() + xlim(c(-2,4.2)) + 
  geom_point(data = dat[!is.na(trainData$X1m),], 
             mapping = aes(x = X1, y = X2),
             colour = "grey", alpha = 0.2)

p2 <- ggplot(df, aes(x = X1med, y = X2, color = Y)) + 
  geom_point(show.legend = TRUE) +
  labs(x = "X1", y = "X2",  title = "medianImpute", color = "Y") + 
  scale_color_gradient(low = "green", high = "red", 
                       na.value = "blue", guide = "legend") +
  theme_minimal() + xlim(c(-2,4.2))

p3 <- ggplot(df, aes(x = X1knn, y = X2, color = Y)) + 
  geom_point(show.legend = TRUE) +
  labs(x = "X1", y = "X2",  title = "knnImpute", color = "Y") + 
  scale_color_gradient(low = "green", high = "red", 
                       na.value = "blue", guide = "legend") +
  theme_minimal() + xlim(c(-2,4.2))

p4 <- ggplot(df, aes(x = X1bag, y = X2, color = Y)) + 
  geom_point(show.legend = TRUE) +
  labs(x = "X1", y = "X2",  title = "bagImpute", color = "Y") + 
  scale_color_gradient(low = "green", high = "red", 
                       na.value = "blue", guide = "legend") +
  theme_minimal() + xlim(c(-2,4.2))

grid.arrange(p1, p2, p3, p4)
```



## `preProcess` in `train()`

```{r}
fit.lm <- train(x = trainData[,c(2,3)],
                y = trainData$Y,
                preProcess = c("knnImpute"), # bagImpute/medianImpute
                method = "lm",
                trControl = trainControl(method = "none",
                                         preProcOptions = list(k = 5)))

pred.lm <- predict(fit.lm, newdata = testData)

mean((testData$Y - pred.lm)^2)
```

