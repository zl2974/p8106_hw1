---
title: "Support Vector Machines"
author: "Yifei Sun"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(mlbench)
library(ISLR)
library(caret)
library(e1071)
library(kernlab)
library(DALEX)
```


# Classification

We use the Pima Indians Diabetes Database for illustration. The data contain 768 observations and 9 variables. The outcome is a binary variable `diabetes`. 

```{r}
data(PimaIndiansDiabetes)
dat <- PimaIndiansDiabetes
dat$diabetes <- factor(dat$diabetes, c("pos", "neg"))

set.seed(2021)
rowTrain <- createDataPartition(y = dat$diabetes,
                                p = 0.75,
                                list = FALSE)
```



## Using `e1071`

Check https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf for more details.

### Linear boundary

Most real data sets will not be fully separable by a linear boundary. Support vector classifiers with a tuning parameter `cost`, which quantifies the penalty associated with having an observation on the wrong side of the classification boundary, can be used to build a linear boundary.

```{r}
set.seed(1)
linear.tune <- tune.svm(diabetes ~ . , 
                        data = dat[rowTrain,], 
                        kernel = "linear", 
                        cost = exp(seq(-5,2,len=50)),
                        scale = TRUE)
plot(linear.tune)
# summary(linear.tune)
linear.tune$best.parameters

best.linear <- linear.tune$best.model
summary(best.linear)

pred.linear <- predict(best.linear, newdata = dat[-rowTrain,])

confusionMatrix(data = pred.linear, 
                reference = dat$diabetes[-rowTrain])

plot(best.linear, dat[rowTrain,], 
     glucose ~ mass,
     slice = list(pregnant = 5, triceps = 20,
                  insulin = 20, pressure = 75,
                  pedigree = 1, age = 50),
     grid = 100)
```

### Radial kernel

Support vector machines can construct classification boundaries that are nonlinear in shape. We use the radial kernel.

```{r}
set.seed(1)
radial.tune <- tune.svm(diabetes ~ . , 
                        data = dat[rowTrain,], 
                        kernel = "radial", 
                        cost = exp(seq(-1,4,len=10)),
                        gamma = exp(seq(-6,-2,len=10)))

plot(radial.tune, transform.y = log, transform.x = log, 
     color.palette = terrain.colors)
# summary(radial.tune)

best.radial <- radial.tune$best.model
summary(best.radial)

pred.radial <- predict(best.radial, newdata = dat[-rowTrain,])

confusionMatrix(data = pred.radial, 
                reference = dat$diabetes[-rowTrain])

plot(best.radial, dat[rowTrain,], 
     glucose ~ mass,
     slice = list(pregnant = 5, triceps = 20,
                  insulin = 20, pressure = 75,
                  pedigree = 1, age = 50),
     grid = 100,
     symbolPalette = c("cyan","darkblue"),
     color.palette = heat.colors)
```   

## Using `kernlab`

Check https://cran.r-project.org/web/packages/kernlab/vignettes/kernlab.pdf for more details.

```{r}
x_train <- as.matrix(dat[rowTrain, 1:8])
x_test <- as.matrix(dat[-rowTrain, 1:8])

linear <- ksvm(x = x_train, 
               y = dat$diabetes[rowTrain], 
               type = "C-svc", 
               kernel = "vanilladot",
               C = 1,
               scaled = TRUE)

pred.linear2 <- predict(linear, newdata = x_test)


# "?dots" for definition of kernel functions 

set.seed(1)
rbf <- ksvm(x = x_train, 
            y = dat$diabetes[rowTrain], 
            type = "C-svc", 
            kernel = "rbfdot" ,
            kpar = "automatic",
            C = 1)

pred.rbf <- predict(rbf, newdata = x_test)
```

## Using `caret`

```{r}
ctrl <- trainControl(method = "cv")

# kernlab
set.seed(1)
svml.fit <- train(diabetes ~ . , 
                  data = dat[rowTrain,], 
                  method = "svmLinear",
                  # preProcess = c("center", "scale"),
                  tuneGrid = data.frame(C = exp(seq(-2,5,len=20))),
                  trControl = ctrl)

plot(svml.fit, highlight = TRUE, xTrans = log)

# e1071
set.seed(1)
svml.fit2 <- train(diabetes ~ . , 
                  data = dat[rowTrain,], 
                  method = "svmLinear2",
                  tuneGrid = data.frame(cost = exp(seq(-2,5,len=20))),
                  trControl = ctrl)

plot(svml.fit2, highlight = TRUE, xTrans = log)
```


```{r}
svmr.grid <- expand.grid(C = exp(seq(-1,4,len=10)),
                         sigma = exp(seq(-8,0,len=10)))

# tunes over both cost and sigma
set.seed(1)             
svmr.fit <- train(diabetes ~ . , dat, 
                  subset = rowTrain,
                  method = "svmRadialSigma",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

plot(svmr.fit, highlight = TRUE)

# tune over cost and uses a single value of sigma based on kernlab's sigest function
set.seed(1)             
svmr.fit2 <- train(diabetes ~ . , dat, 
                   subset = rowTrain,
                   method = "svmRadialCost",
                   preProcess = c("center", "scale"),
                   tuneGrid = data.frame(C = exp(seq(-4,2,len=10))),
                   trControl = ctrl)

# Platt’s probabilistic outputs; use with caution
set.seed(1)             
svmr.fit3 <- train(diabetes ~ . , dat, 
                   subset = rowTrain,
                   method = "svmRadialCost",
                   preProcess = c("center", "scale"),
                   tuneGrid = data.frame(C = exp(seq(-4,2,len=10))),
                   trControl = ctrl,
                   prob.model = TRUE) 
# predict(svmr.fit3, newdata = x_test, type = "prob")

set.seed(1)             
rpart.fit <- train(diabetes ~ . , dat, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneLength = 50,
                   trControl = ctrl)
```

```{r}
resamp <- resamples(list(svmr = svmr.fit, svmr2 = svmr.fit2,
                         svml = svml.fit, svml2 = svml.fit2,
                         rpart = rpart.fit))
bwplot(resamp)
```

We finally look at the test data performance.
```{r}
pred.svml <- predict(svml.fit, newdata = dat[-rowTrain,])
pred.svmr <- predict(svmr.fit, newdata = dat[-rowTrain,])

confusionMatrix(data = pred.svml, 
                reference = dat$diabetes[-rowTrain])

confusionMatrix(data = pred.svmr, 
                reference = dat$diabetes[-rowTrain])
```

## Understanding your models with `DALEX`

```{r}
explainer_rpart <- explain(rpart.fit, 
                           label = "rpart", 
                           data = x_train,
                           y = as.numeric(dat$diabetes[rowTrain] == "pos"),
                           verbose = FALSE)

# SVM does not output probabilities, this is using Platt's output and is just for illustration of DALEX
explainer_svm <- explain(svmr.fit3, 
                         label = "svmr", 
                         data = x_train,
                         y = as.numeric(dat$diabetes[rowTrain] == "pos"),
                         verbose = FALSE)

# variable importance
vi_rpart <- model_parts(explainer_rpart)
vi_svm <- model_parts(explainer_svm)

plot(vi_rpart, vi_svm)

# PDP
pdp_svm <- model_profile(explainer_svm, 
                         variable = "glucose", 
                         type = "partial")
pdp_rpart <- model_profile(explainer_rpart, 
                           variable = "glucose", 
                           type = "partial")
plot(pdp_svm, pdp_rpart)

# bread down
pb_svm <- predict_parts(explainer_svm, 
                        new_observation = dat[1,], 
                        type = "break_down")
pb_rpart <- predict_parts(explainer_rpart, 
                          new_observation = dat[1,], 
                          type = "break_down")
plot(pb_rpart)
plot(pb_svm)
```

# Regression

Predict a baseball player’s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details.

```{r}
data(Hitters)
Hitters <- na.omit(Hitters)

set.seed(2021)
trRows <- createDataPartition(Hitters$Salary,
                              p = .75,
                              list = F)
```

## `eps-regression`

```{r}
set.seed(1)
svml.fit <- tune.svm(Salary ~ . , 
                     data = Hitters[trRows,], 
                     kernel = "linear", 
                     epsilon = exp(seq(-5,0,len=20)))

set.seed(1)
svmr.fit <- tune.svm(Salary ~ . , 
                     data = Hitters[trRows,], 
                     kernel = "radial", 
                     epsilon = exp(seq(-5,0,len=10)),
                     gamma = exp(seq(-6,-2,len=10)))

svmr.pred <- predict(svmr.fit$best.model, newdata = Hitters[-trRows,])
RMSE(svmr.pred, Hitters$Salary[-trRows])
```
