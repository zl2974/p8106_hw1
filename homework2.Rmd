---
title: "Homework 2"
author: "Jeffrey LIANG"
date: "2/20/2021"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(glmnet)
library(pls)
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(doParallel)

knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  echo = T,
  warning = F,
  cache = T
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)


scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

```{r}
set.seed(123123)
```

# Q1

```{r echo = F}
clg_data =
  read_csv("College.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    college = as.factor(college)
  )

skimr::skim_without_charts(clg_data)
```

Missing data is the respone, omitting the data instead of treating with data preprocessing.

```{r data_preprocess}
clg_data = clg_data %>% drop_na()

clg_train = clg_data

Y_train = clg_train$outstate

X_train = model.matrix(outstate ~., data = clg_train)[,-1]

ctrl = trainControl(method = "repeatedcv",number = 5, repeats = 5)

clg_data %>% 
  select(-college,-outstate) %>% 
  featurePlot(.,clg_data$outstate,plot = "scatter",row = 4)
```



# Q2

```{r smooth_spline}
set.seed(123123)
clg_ss_cv = smooth.spline(clg_train$terminal, Y_train, cv = T)

clg_ss_cv_mse = mean((predict(clg_ss_cv,clg_train$terminal,se=F)$y-Y_train)^2)

clg_ss =
  tibble(
    x = list(clg_train$terminal),
    y = list(Y_train),
    df = list(seq(2, 20, length = 5)%/%1)
  ) %>%
  unnest(df) %>%
  mutate(model = pmap(list(x, y, df),
                      function(x, y, df, ...)
                        smooth.spline(
                          x = x, y = y, df = df
                        ))) %>%
  rbind(list(
    x = list(clg_train$terminal),
    y = list(Y_train),
    df = clg_ss_cv$df,
    model = list(clg_ss_cv)
  )) %>%
  mutate(
    prediction = map2(.x = x,
                      .y = model,
                      ~predict(object = .y,x = .x,se=F)$y),
    df = as.factor(df)
  ) %>%
  select(df, y, prediction, x) %>%
  unnest(c(prediction, y,x))

clg_ss %>%
  group_by(df) %>%
  summarise(mse =
              mean((y - prediction) ^ 2)) %>% 
  knitr::kable(caption = "Smooth spline performance with different degree of freedom",digits = 3)

ggplot(clg_ss) +
  geom_point(aes(x = x, y = y),alpha = 0.05) +
  geom_line(aes(x = x, y = prediction, color= df)) +
  facet_wrap(df ~ ., nrow = 2) +
  labs(title = "Smooth Spline")
```



The model obtained from CV method has the degree of freedom of `r clg_ss_cv$df` and lambda `r clg_ss_cv$lambda` has the lowest MSE in the model candidates. The fitted model is almost a smooth line. The $MSE_{tr}$ is `r clg_ss_cv_mse`.



# Q3
```{r gam, cache=T}
set.seed(123123)
cl = makePSOCKcluster(5)# if windows, set to 1

registerDoParallel(cl)

clg_gam =
  train(
    x = X_train,
    y = Y_train,
    method = "gam",
    tuneGrid = expand.grid(select = c(T, F),
                           method  = c("GCV.cp", "REML")),
    metric = "RMSE",
    trControl = ctrl
  )

stopCluster(cl)

clg_gam$bestTune

clg_gam_mse = mean((Y_train-predict(clg_gam))^2)

summary(clg_gam$finalModel)

par(mfrow = c(2,4))

plot(clg_gam$finalModel)

par(mfrow=c(2,2))
for (i in 1:8){
  predictor = clg_gam$finalModel$terms %>% attr("term.labels") %>% .[(2*i-1):(2*i)]
  vis.gam(clg_gam$finalModel,predictor)
}
```

Using caret tuning, the best tuning methods is `select = F` and `method = "REML"`. With this method, all variable is applied with spline function except for Indicator of `College` which is not selected by caret. The $MSE_{tr}$ is `r clg_gam_mse`.

# Q4

```{r mars,cache=T}
set.seed(123123)
cl = makePSOCKcluster(5) #if windows, set to 1
registerDoParallel(cl)
clg_mars =
  train(
    x = X_train,
    y = Y_train,
    method = "earth",
    tuneGrid = expand.grid(degree = 1:3,
                           nprune = exp(
                             seq(1, log(100), length = 10)
                           )%/%1),
    metric = "RMSE",
    trControl = ctrl
  )
stopCluster(cl)

clg_mars$finalModel$coefficients %>% 
  knitr::kable(caption = "Hints")

ggplot(clg_mars)

clg_mars$bestTune

summary(clg_mars$finalModel)

p1 = pdp::partial(clg_mars, pred.var = c("grad_rate", "f_undergrad")) %>%
  plotPartial(
    levelplot = FALSE,
    zlab = "yhat",
    drape = TRUE,
    screen = list(z = 20, x = -60)
  )

p2 = pdp::partial(clg_mars, pred.var = c("apps", "enroll")) %>%
  plotPartial(
    levelplot = FALSE,
    zlab = "yhat",
    drape = TRUE,
    screen = list(z = 20, x = -60)
  )

grid.arrange(p1,p2,nrow = 2)
```

The final model has 3 degree and 30 hints in the model. total of 30 term and 26 predictors are includes in the model. The mse of the MARS model is `r mean((Y_train - predict(clg_mars))^2)`

```{r resample}
rmp = caret::resamples(list(gam = clg_gam,
                            mars = clg_mars))

summary(rmp)
```


